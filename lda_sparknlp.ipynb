{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lda-sparknlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNiSvGiuw17GmPy3iFtVNAP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartikarizqin/mtsamples-analysis/blob/main/lda_sparknlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic Modeling (LDA) with SparkNLP**\n",
        "\n",
        "Reference: https://medium.com/trustyou-engineering/topic-modelling-with-pyspark-and-spark-nlp-a99d063f1a6e (with adjustment in some lines)"
      ],
      "metadata": {
        "id": "bnJS3PF1z85-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.5\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp==2.4.5\n",
        "\n",
        "# Install nltk\n",
        "! pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtP1_C0Atjbl",
        "outputId": "eb9722d2-f8c4-43cb-c9f7-37924385c7be"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_312\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~18.04-b07)\n",
            "OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\n",
            "Collecting pyspark==2.4.5\n",
            "  Using cached pyspark-2.4.5-py2.py3-none-any.whl\n",
            "Collecting py4j==0.10.7\n",
            "  Using cached py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n",
            "Collecting spark-nlp==2.4.5\n",
            "  Using cached spark_nlp-2.4.5-py2.py3-none-any.whl (110 kB)\n",
            "Installing collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.4.5\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "F2v944i6lucj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dadc7177-27e0-4e5a-f955-195e15439215"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZwLe5o3KtS2Q"
      },
      "outputs": [],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "DY10aHtgtYBn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/psy_cleaned.csv'\n",
        "data = spark.read.csv(data_path, header=True)"
      ],
      "metadata": {
        "id": "qfhklMt2v9T8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Kq59QjnwOhf",
        "outputId": "2f556418-781f-43fe-b981-2c67b5046336"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_col = 'text'\n",
        "review_text = data.select(text_col).filter(F.col(text_col).isNotNull())"
      ],
      "metadata": {
        "id": "cGzvHQWnwQn6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_text.limit(5).show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhhV2XmqwW4Y",
        "outputId": "2fe4d3e1-ad25-4303-9d30-5af009b8550a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                                      text|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|adjustment disorder encopresis patient referred due concerns regarding behavioral actin...|\n",
            "|agitation er visit acute episode agitation complaining felt might poisoned care facilit...|\n",
            "|asperger disorder school reports continuing difficulties repetitive questioning obsessi...|\n",
            "|attempted suicide consult patient year old caucasian male attempted suicide trying jump...|\n",
            "|bipolar affective disorder consult patient manic disorder presently psychotic flight id...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "\n",
        "documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol(text_col) \\\n",
        "     .setOutputCol('document')"
      ],
      "metadata": {
        "id": "9GQkXLkwwkJs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "     .setInputCols(['document']) \\\n",
        "     .setOutputCol('tokenized')"
      ],
      "metadata": {
        "id": "ltJ-uFIEwsBc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import Normalizer\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "     .setInputCols(['tokenized']) \\\n",
        "     .setOutputCol('normalized') \\\n",
        "     .setLowercase(True)"
      ],
      "metadata": {
        "id": "XTUZS646wt3R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import LemmatizerModel\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained() \\\n",
        "     .setInputCols(['normalized']) \\\n",
        "     .setOutputCol('lemmatized')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMlqcC2ywv_l",
        "outputId": "7d8f0ccf-39e6-4a4b-ea68-5b6bbd81ed23"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "eng_stopwords = stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nadoPNkTwx2_",
        "outputId": "7f581df6-e162-4f41-d039-c085fa57561f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import StopWordsCleaner\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner() \\\n",
        "     .setInputCols(['lemmatized']) \\\n",
        "     .setOutputCol('unigrams') \\\n",
        "     .setStopWords(eng_stopwords)"
      ],
      "metadata": {
        "id": "q1Lbgw5Vw0A4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import NGramGenerator\n",
        "\n",
        "ngrammer = NGramGenerator() \\\n",
        "    .setInputCols(['lemmatized']) \\\n",
        "    .setOutputCol('ngrams') \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter('_')"
      ],
      "metadata": {
        "id": "qQUYjTtew1rS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import PerceptronModel\n",
        "\n",
        "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
        "    .setInputCols(['document', 'lemmatized']) \\\n",
        "    .setOutputCol('pos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m9M2O6Fw3ch",
        "outputId": "255f1e33-a2c0-4c10-ef75-cab69c62ccaa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import Finisher\n",
        "\n",
        "finisher = Finisher() \\\n",
        "     .setInputCols(['unigrams', 'ngrams', 'pos']) \\"
      ],
      "metadata": {
        "id": "PF4iia-Mw5Xq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline() \\\n",
        "     .setStages([documentAssembler,                  \n",
        "                 tokenizer,\n",
        "                 normalizer,                  \n",
        "                 lemmatizer,                  \n",
        "                 stopwords_cleaner, \n",
        "                 pos_tagger,\n",
        "                 ngrammer,  \n",
        "                 finisher])"
      ],
      "metadata": {
        "id": "Z2rwddFkw8Gd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review = pipeline.fit(review_text).transform(review_text)"
      ],
      "metadata": {
        "id": "-gSJEgR3w9oK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.limit(5).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "as0fP7Cgw_w2",
        "outputId": "0e8c103c-5adf-4612-8a74-c54a1ad1fd6f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|adjustment disord...|[adjustment, diso...|[adjustment, diso...|[NN, NN, NN, NN, ...|\n",
            "|agitation er visi...|[agitation, er, v...|[agitation, er, v...|[NN, UH, NN, JJ, ...|\n",
            "|asperger disorder...|[asperger, disord...|[asperger, disord...|[NN, NN, NN, NN, ...|\n",
            "|attempted suicide...|[attempt, suicide...|[attempt, suicide...|[NN, NN, NN, NN, ...|\n",
            "|bipolar affective...|[bipolar, affecti...|[bipolar, affecti...|[NN, JJ, NN, NN, ...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import types as T\n",
        "\n",
        "udf_join_arr = F.udf(lambda x: ' '.join(x), T.StringType())\n",
        "processed_review  = processed_review.withColumn('finished_pos', udf_join_arr(F.col('finished_pos')))"
      ],
      "metadata": {
        "id": "celTpR1IxBWx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol('finished_pos') \\\n",
        "     .setOutputCol('pos_document')"
      ],
      "metadata": {
        "id": "rPcQlV28xD57"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tokenizer = Tokenizer() \\\n",
        "     .setInputCols(['pos_document']) \\\n",
        "     .setOutputCol('pos')"
      ],
      "metadata": {
        "id": "qyZNZYOqxGj4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ngrammer = NGramGenerator() \\\n",
        "    .setInputCols(['pos']) \\\n",
        "    .setOutputCol('pos_ngrams') \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter('_')"
      ],
      "metadata": {
        "id": "lbBcUpk6xJ8F"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_finisher = Finisher() \\\n",
        "     .setInputCols(['pos', 'pos_ngrams']) \\"
      ],
      "metadata": {
        "id": "_w7nOgqaxLjv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_pipeline = Pipeline().setStages([pos_documentAssembler, pos_tokenizer, pos_ngrammer, pos_finisher])"
      ],
      "metadata": {
        "id": "MySKgZNcxNGt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)"
      ],
      "metadata": {
        "id": "95b1MTfpxOuS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.columns"
      ],
      "metadata": {
        "id": "Tcn_fRptxQTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45e5337b-82f8-42ce-81b5-22677d413901"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text',\n",
              " 'finished_unigrams',\n",
              " 'finished_ngrams',\n",
              " 'finished_pos',\n",
              " 'finished_pos_ngrams']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.select('finished_ngrams', 'finished_pos_ngrams').limit(5).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTdTenDszeJ8",
        "outputId": "75afebc7-0dec-4053-cf6b-df5d6f906533"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|     finished_ngrams| finished_pos_ngrams|\n",
            "+--------------------+--------------------+\n",
            "|[adjustment, diso...|[NN, NN, NN, NN, ...|\n",
            "|[agitation, er, v...|[NN, UH, NN, JJ, ...|\n",
            "|[asperger, disord...|[NN, NN, NN, NN, ...|\n",
            "|[attempt, suicide...|[NN, NN, NN, NN, ...|\n",
            "|[bipolar, affecti...|[NN, JJ, NN, NN, ...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_pos(words, pos_tags):\n",
        "    return [word for word, pos in zip(words, pos_tags) \n",
        "            if pos in ['JJ', 'NN', 'NNS', 'VB', 'VBP']]\n",
        "\n",
        "udf_filter_pos = F.udf(filter_pos, T.ArrayType(T.StringType()))"
      ],
      "metadata": {
        "id": "SVD5RqhHzgQl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review = processed_review.withColumn('filtered_unigrams',\n",
        "                                               udf_filter_pos(F.col('finished_unigrams'), \n",
        "                                                              F.col('finished_pos')))"
      ],
      "metadata": {
        "id": "-xN3UBaSziHj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.select('filtered_unigrams').limit(5).show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ99HXxZzj7o",
        "outputId": "10e1e47c-439d-4887-a794-f8ef04d374f2"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                         filtered_unigrams|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[adjustment, disorder, encopresis, patient, refer, due, concern, regard, behavioral, ac...|\n",
            "|[agitation, visit, acute, episode, agitation, complain, feel, poison, care, facility, c...|\n",
            "|[asperger, disorder, school, report, continue, difficulty, repetitive, question, obsess...|\n",
            "|[attempt, suicide, consult, patient, year, old, caucasian, male, attempt, suicide, try,...|\n",
            "|[bipolar, affective, disorder, consult, patient, manic, disorder, psychotic, flight, id...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_pos_combs(words, pos_tags):\n",
        "    return [word for word, pos in zip(words, pos_tags) \n",
        "            if (len(pos.split('_')) == 2 and \\\n",
        "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n",
        "            or (len(pos.split('_')) == 3 and \\\n",
        "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                  pos.split('_')[2] in ['NN', 'NNS'])]\n",
        "    \n",
        "udf_filter_pos_combs = F.udf(filter_pos_combs, T.ArrayType(T.StringType()))"
      ],
      "metadata": {
        "id": "ABqZT2WazmUi"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review = processed_review.withColumn('filtered_ngrams',\n",
        "                                               udf_filter_pos_combs(F.col('finished_ngrams'),\n",
        "                                                                    F.col('finished_pos_ngrams')))"
      ],
      "metadata": {
        "id": "Ng9umXVTzoPb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.select('filtered_ngrams').limit(5).show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjID-Pq1zqdx",
        "outputId": "d506fbc2-0ff6-4824-bfb2-e608be7a5294"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                           filtered_ngrams|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[adjustment_disorder, disorder_encopresis, encopresis_patient, refer_due, due_concern, ...|\n",
            "|[visit_acute, acute_episode, episode_agitation, agitation_complain, poison_care, care_f...|\n",
            "|[asperger_disorder, disorder_school, school_report, continue_difficulty, difficulty_rep...|\n",
            "|[attempt_suicide, suicide_consult, consult_patient, patient_year, year_old, old_caucasi...|\n",
            "|[bipolar_affective, affective_disorder, disorder_consult, consult_patient, patient_mani...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat\n",
        "\n",
        "processed_review = processed_review.withColumn('final', \n",
        "                                               concat(F.col('filtered_unigrams'), \n",
        "                                                      F.col('filtered_ngrams')))"
      ],
      "metadata": {
        "id": "OH9fbNruzsLZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.select('final').limit(5).show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiHjLZqCzuMb",
        "outputId": "95f4b114-3334-42be-f543-527de73eae3c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                                     final|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[adjustment, disorder, encopresis, patient, refer, due, concern, regard, behavioral, ac...|\n",
            "|[agitation, visit, acute, episode, agitation, complain, feel, poison, care, facility, c...|\n",
            "|[asperger, disorder, school, report, continue, difficulty, repetitive, question, obsess...|\n",
            "|[attempt, suicide, consult, patient, year, old, caucasian, male, attempt, suicide, try,...|\n",
            "|[bipolar, affective, disorder, consult, patient, manic, disorder, psychotic, flight, id...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "tfizer = CountVectorizer(inputCol='final', outputCol='tf_features')\n",
        "tf_model = tfizer.fit(processed_review)\n",
        "tf_result = tf_model.transform(processed_review)"
      ],
      "metadata": {
        "id": "D2zZUI3-zwCh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import IDF\n",
        "\n",
        "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
        "idf_model = idfizer.fit(tf_result)\n",
        "tfidf_result = idf_model.transform(tf_result)"
      ],
      "metadata": {
        "id": "7K9tmFPjzyVk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "num_topics = 6\n",
        "max_iter = 10\n",
        "\n",
        "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features')\n",
        "lda_model = lda.fit(tfidf_result)"
      ],
      "metadata": {
        "id": "uq2wk725zz_m"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tf_model.vocabulary\n",
        "\n",
        "def get_words(token_list):\n",
        "     return [vocab[token_id] for token_id in token_list]\n",
        "       \n",
        "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
      ],
      "metadata": {
        "id": "aavwJtlzz2Kr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_top_words = 10\n",
        "\n",
        "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
        "topics.select('topic', 'topicWords').show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQCzmZJYz4Gv",
        "outputId": "e51f2604-4c95-4aa9-b1f5-147b9ef5176d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------------------------------------------------------------------------------+\n",
            "|topic|                                                                                topicWords|\n",
            "+-----+------------------------------------------------------------------------------------------+\n",
            "|    0|[mg_p, schizoaffective, schizoaffective_disorder, mg, p, pain, huntington, unable, with...|\n",
            "|    1|[posttraumatic, posttraumatic_stress, performance, stress, stress_disorder, test, postt...|\n",
            "|    2|[evidence, state, excessive, poor, suicide, report, wife, employee, disease, major_depr...|\n",
            "|    3|[p, craniotomy, mg_p, status_change, unresponsive, q, mental_status_change, q_h, h, inf...|\n",
            "|    4|[patient_state, therapist, state, husband, anger, borderline_personality, personality_d...|\n",
            "|    5|[migraine, dizziness, headache, syncope, neuropsychological, migraine_symptom, neuropat...|\n",
            "+-----+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bo8kFpLSz57T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}