{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lda-sparknlp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNK/4YM7fpGPuAKcF5WLBel",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartikarizqin/mtsamples-analysis/blob/main/lda_sparknlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic Modeling (LDA) with SparkNLP**\n",
        "\n",
        "Reference: https://medium.com/trustyou-engineering/topic-modelling-with-pyspark-and-spark-nlp-a99d063f1a6e (with adjustment in some lines)"
      ],
      "metadata": {
        "id": "bnJS3PF1z85-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.5\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp==2.4.5\n",
        "\n",
        "# Install nltk\n",
        "! pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JtP1_C0Atjbl",
        "outputId": "9105a6b7-c19a-4472-caa7-4b66135c4028"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "openjdk version \"1.8.0_312\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_312-8u312-b07-0ubuntu1~18.04-b07)\n",
            "OpenJDK 64-Bit Server VM (build 25.312-b07, mixed mode)\n",
            "Collecting pyspark==2.4.5\n",
            "  Downloading pyspark-2.4.5.tar.gz (217.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8 MB 7.6 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 24.6 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257928 sha256=9a669b8e84ec175df7dd8290c20bf0a671fe3d658e12d6bda4229815a6ee3a4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/03/1c241c9c482b647d4d99412a98a5c7f87472728ad41ae55e1e\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n",
            "Collecting spark-nlp==2.4.5\n",
            "  Downloading spark_nlp-2.4.5-py2.py3-none-any.whl (110 kB)\n",
            "\u001b[K     |████████████████████████████████| 110 kB 14.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.4.5\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "F2v944i6lucj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "6055043a-c585-44d4-d712-c0bf845081a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZwLe5o3KtS2Q"
      },
      "outputs": [],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "DY10aHtgtYBn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/psy_cleaned.csv'\n",
        "data = spark.read.csv(data_path, header=True)"
      ],
      "metadata": {
        "id": "qfhklMt2v9T8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5Kq59QjnwOhf",
        "outputId": "bd4d6c1a-3bba-4c46-fa42-3629e08bc6c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# try spark sql function\n",
        "\n",
        "text_col = 'text'\n",
        "review_text = data.select(text_col).filter(F.col(text_col).isNotNull())"
      ],
      "metadata": {
        "id": "cGzvHQWnwQn6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_text.limit(5).show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WhhV2XmqwW4Y",
        "outputId": "3f69680a-6f31-4d16-eb32-9ef9558e4794"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                                      text|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|adjustment disorder encopresis patient referred due concerns regarding behavioral actin...|\n",
            "|agitation er visit acute episode agitation complaining felt might poisoned care facilit...|\n",
            "|asperger disorder school reports continuing difficulties repetitive questioning obsessi...|\n",
            "|attempted suicide consult patient year old caucasian male attempted suicide trying jump...|\n",
            "|bipolar affective disorder consult patient manic disorder presently psychotic flight id...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "\n",
        "documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol(text_col) \\\n",
        "     .setOutputCol('document')"
      ],
      "metadata": {
        "id": "9GQkXLkwwkJs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "     .setInputCols(['document']) \\\n",
        "     .setOutputCol('tokenized')"
      ],
      "metadata": {
        "id": "ltJ-uFIEwsBc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import Normalizer\n",
        "\n",
        "normalizer = Normalizer() \\\n",
        "     .setInputCols(['tokenized']) \\\n",
        "     .setOutputCol('normalized') \\\n",
        "     .setLowercase(True)"
      ],
      "metadata": {
        "id": "XTUZS646wt3R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import LemmatizerModel\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained() \\\n",
        "     .setInputCols(['normalized']) \\\n",
        "     .setOutputCol('lemmatized')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QMlqcC2ywv_l",
        "outputId": "11d210c7-4cee-4a3d-e22c-16b0423b70f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "eng_stopwords = stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nadoPNkTwx2_",
        "outputId": "89a132fa-232e-4858-a14a-5f1181c82301"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import StopWordsCleaner\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner() \\\n",
        "     .setInputCols(['lemmatized']) \\\n",
        "     .setOutputCol('unigrams') \\\n",
        "     .setStopWords(eng_stopwords)"
      ],
      "metadata": {
        "id": "q1Lbgw5Vw0A4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import NGramGenerator\n",
        "\n",
        "ngrammer = NGramGenerator() \\\n",
        "    .setInputCols(['lemmatized']) \\\n",
        "    .setOutputCol('ngrams') \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter('_')"
      ],
      "metadata": {
        "id": "qQUYjTtew1rS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.annotator import PerceptronModel\n",
        "\n",
        "pos_tagger = PerceptronModel.pretrained('pos_anc') \\\n",
        "    .setInputCols(['document', 'lemmatized']) \\\n",
        "    .setOutputCol('pos')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_m9M2O6Fw3ch",
        "outputId": "338e130b-5afb-4a1a-804b-11d6445dbf22"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 4.3 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import Finisher\n",
        "\n",
        "finisher = Finisher() \\\n",
        "     .setInputCols(['unigrams', 'ngrams', 'pos']) \\"
      ],
      "metadata": {
        "id": "PF4iia-Mw5Xq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline() \\\n",
        "     .setStages([documentAssembler,                  \n",
        "                 tokenizer,\n",
        "                 normalizer,                  \n",
        "                 lemmatizer,                  \n",
        "                 stopwords_cleaner, \n",
        "                 pos_tagger,\n",
        "                 ngrammer,  \n",
        "                 finisher])"
      ],
      "metadata": {
        "id": "Z2rwddFkw8Gd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review = pipeline.fit(review_text).transform(review_text)"
      ],
      "metadata": {
        "id": "-gSJEgR3w9oK"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.limit(5).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "as0fP7Cgw_w2",
        "outputId": "33417f8a-67c2-4c25-9f3a-2e402d177860"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|   finished_unigrams|     finished_ngrams|        finished_pos|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "|adjustment disord...|[adjustment, diso...|[adjustment, diso...|[NN, NN, NN, NN, ...|\n",
            "|agitation er visi...|[agitation, er, v...|[agitation, er, v...|[NN, UH, NN, JJ, ...|\n",
            "|asperger disorder...|[asperger, disord...|[asperger, disord...|[NN, NN, NN, NN, ...|\n",
            "|attempted suicide...|[attempt, suicide...|[attempt, suicide...|[NN, NN, NN, NN, ...|\n",
            "|bipolar affective...|[bipolar, affecti...|[bipolar, affecti...|[NN, JJ, NN, NN, ...|\n",
            "+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import types as T\n",
        "\n",
        "udf_join_arr = F.udf(lambda x: ' '.join(x), T.StringType())\n",
        "processed_review  = processed_review.withColumn('finished_pos', udf_join_arr(F.col('finished_pos')))"
      ],
      "metadata": {
        "id": "celTpR1IxBWx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_documentAssembler = DocumentAssembler() \\\n",
        "     .setInputCol('finished_pos') \\\n",
        "     .setOutputCol('pos_document')"
      ],
      "metadata": {
        "id": "rPcQlV28xD57"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tokenizer = Tokenizer() \\\n",
        "     .setInputCols(['pos_document']) \\\n",
        "     .setOutputCol('pos')"
      ],
      "metadata": {
        "id": "qyZNZYOqxGj4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_ngrammer = NGramGenerator() \\\n",
        "    .setInputCols(['pos']) \\\n",
        "    .setOutputCol('pos_ngrams') \\\n",
        "    .setN(3) \\\n",
        "    .setEnableCumulative(True) \\\n",
        "    .setDelimiter('_')"
      ],
      "metadata": {
        "id": "lbBcUpk6xJ8F"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_finisher = Finisher() \\\n",
        "     .setInputCols(['pos', 'pos_ngrams']) \\"
      ],
      "metadata": {
        "id": "_w7nOgqaxLjv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_pipeline = Pipeline().setStages([pos_documentAssembler, pos_tokenizer, pos_ngrammer, pos_finisher])"
      ],
      "metadata": {
        "id": "MySKgZNcxNGt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review = pos_pipeline.fit(processed_review).transform(processed_review)"
      ],
      "metadata": {
        "id": "95b1MTfpxOuS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.columns"
      ],
      "metadata": {
        "id": "Tcn_fRptxQTJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "03e6d2e3-75bf-4a56-bf03-80e3aa9450b9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['text',\n",
              " 'finished_unigrams',\n",
              " 'finished_ngrams',\n",
              " 'finished_pos',\n",
              " 'finished_pos_ngrams']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.select('finished_ngrams', 'finished_pos_ngrams').limit(5).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HTdTenDszeJ8",
        "outputId": "14eb8544-2a11-4e22-a618-d4de1fd261fd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+\n",
            "|     finished_ngrams| finished_pos_ngrams|\n",
            "+--------------------+--------------------+\n",
            "|[adjustment, diso...|[NN, NN, NN, NN, ...|\n",
            "|[agitation, er, v...|[NN, UH, NN, JJ, ...|\n",
            "|[asperger, disord...|[NN, NN, NN, NN, ...|\n",
            "|[attempt, suicide...|[NN, NN, NN, NN, ...|\n",
            "|[bipolar, affecti...|[NN, JJ, NN, NN, ...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_pos(words, pos_tags):\n",
        "    return [word for word, pos in zip(words, pos_tags) \n",
        "            if pos in ['JJ', 'NN', 'NNS', 'VB', 'VBP']]\n",
        "\n",
        "udf_filter_pos = F.udf(filter_pos, T.ArrayType(T.StringType()))"
      ],
      "metadata": {
        "id": "SVD5RqhHzgQl"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review = processed_review.withColumn('filtered_unigrams',\n",
        "                                               udf_filter_pos(F.col('finished_unigrams'), \n",
        "                                                              F.col('finished_pos')))"
      ],
      "metadata": {
        "id": "-xN3UBaSziHj"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.select('filtered_unigrams').limit(5).show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vJ99HXxZzj7o",
        "outputId": "f72967b1-e598-4814-8018-0ac8135bae23"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                         filtered_unigrams|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[adjustment, disorder, encopresis, patient, refer, due, concern, regard, behavioral, ac...|\n",
            "|[agitation, visit, acute, episode, agitation, complain, feel, poison, care, facility, c...|\n",
            "|[asperger, disorder, school, report, continue, difficulty, repetitive, question, obsess...|\n",
            "|[attempt, suicide, consult, patient, year, old, caucasian, male, attempt, suicide, try,...|\n",
            "|[bipolar, affective, disorder, consult, patient, manic, disorder, psychotic, flight, id...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_pos_combs(words, pos_tags):\n",
        "    return [word for word, pos in zip(words, pos_tags) \n",
        "            if (len(pos.split('_')) == 2 and \\\n",
        "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS']) \\\n",
        "            or (len(pos.split('_')) == 3 and \\\n",
        "                pos.split('_')[0] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                 pos.split('_')[1] in ['JJ', 'NN', 'NNS', 'VB', 'VBP'] and \\\n",
        "                  pos.split('_')[2] in ['NN', 'NNS'])]\n",
        "    \n",
        "udf_filter_pos_combs = F.udf(filter_pos_combs, T.ArrayType(T.StringType()))"
      ],
      "metadata": {
        "id": "ABqZT2WazmUi"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review = processed_review.withColumn('filtered_ngrams',\n",
        "                                               udf_filter_pos_combs(F.col('finished_ngrams'),\n",
        "                                                                    F.col('finished_pos_ngrams')))"
      ],
      "metadata": {
        "id": "Ng9umXVTzoPb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.select('filtered_ngrams').limit(5).show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HjID-Pq1zqdx",
        "outputId": "ba625d74-f361-4c96-fd38-3495d984b3ca"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                           filtered_ngrams|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[adjustment_disorder, disorder_encopresis, encopresis_patient, refer_due, due_concern, ...|\n",
            "|[visit_acute, acute_episode, episode_agitation, agitation_complain, poison_care, care_f...|\n",
            "|[asperger_disorder, disorder_school, school_report, continue_difficulty, difficulty_rep...|\n",
            "|[attempt_suicide, suicide_consult, consult_patient, patient_year, year_old, old_caucasi...|\n",
            "|[bipolar_affective, affective_disorder, disorder_consult, consult_patient, patient_mani...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import concat\n",
        "\n",
        "processed_review = processed_review.withColumn('final', \n",
        "                                               concat(F.col('filtered_unigrams'), \n",
        "                                                      F.col('filtered_ngrams')))"
      ],
      "metadata": {
        "id": "OH9fbNruzsLZ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_review.select('final').limit(5).show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EiHjLZqCzuMb",
        "outputId": "78eaf0b1-3fb4-4a6d-a4f9-a4c0b0502cfe"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|                                                                                     final|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[adjustment, disorder, encopresis, patient, refer, due, concern, regard, behavioral, ac...|\n",
            "|[agitation, visit, acute, episode, agitation, complain, feel, poison, care, facility, c...|\n",
            "|[asperger, disorder, school, report, continue, difficulty, repetitive, question, obsess...|\n",
            "|[attempt, suicide, consult, patient, year, old, caucasian, male, attempt, suicide, try,...|\n",
            "|[bipolar, affective, disorder, consult, patient, manic, disorder, psychotic, flight, id...|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "tfizer = CountVectorizer(inputCol='final', outputCol='tf_features')\n",
        "tf_model = tfizer.fit(processed_review)\n",
        "tf_result = tf_model.transform(processed_review)"
      ],
      "metadata": {
        "id": "D2zZUI3-zwCh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import IDF\n",
        "\n",
        "idfizer = IDF(inputCol='tf_features', outputCol='tf_idf_features')\n",
        "idf_model = idfizer.fit(tf_result)\n",
        "tfidf_result = idf_model.transform(tf_result)"
      ],
      "metadata": {
        "id": "7K9tmFPjzyVk"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "num_topics = 6\n",
        "max_iter = 10\n",
        "\n",
        "lda = LDA(k=num_topics, maxIter=max_iter, featuresCol='tf_idf_features')\n",
        "lda_model = lda.fit(tfidf_result)"
      ],
      "metadata": {
        "id": "uq2wk725zz_m"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = tf_model.vocabulary\n",
        "\n",
        "def get_words(token_list):\n",
        "     return [vocab[token_id] for token_id in token_list]\n",
        "       \n",
        "udf_to_words = F.udf(get_words, T.ArrayType(T.StringType()))"
      ],
      "metadata": {
        "id": "aavwJtlzz2Kr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_top_words = 10\n",
        "\n",
        "topics = lda_model.describeTopics(num_top_words).withColumn('topicWords', udf_to_words(F.col('termIndices')))\n",
        "topics.select('topic', 'topicWords').show(truncate=90)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KQCzmZJYz4Gv",
        "outputId": "105e874b-7b5b-4a8a-8e40-52a378630380"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------------------------------------------------------------------------------+\n",
            "|topic|                                                                                topicWords|\n",
            "+-----+------------------------------------------------------------------------------------------+\n",
            "|    0|[pain, migraine, attention, cymbalta, attention_deficit, age, currently, mother_report,...|\n",
            "|    1|[discharge, rtc, discharge_summary, involuntary, bipolar_affective, assaultive, opposit...|\n",
            "|    2|[encopresis, oppositionality, mother_report, report, regard, mother, migraine, dizzines...|\n",
            "|    3|[bipolar_affective, bipolar_affective_disorder, affective_disorder, mother, abcd, affec...|\n",
            "|    4|     [mg_p, p, unknown, craniotomy, daily, p_daily, psychosis, headache, mg, glioblastoma]|\n",
            "|    5|[edition, test, average_range, verbal, range, mild, task, error, neuropsychological, wi...|\n",
            "+-----+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bo8kFpLSz57T"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}